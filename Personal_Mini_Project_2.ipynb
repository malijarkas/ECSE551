{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "QzlY-Rt3D9BZ",
        "ff4Rg051q4sh",
        "L31DcySpyqLi",
        "HQ6oxKyjc2S6",
        "ovQt8kl08ton"
      ],
      "mount_file_id": "1D2x-n5Jyxaaet4eTXGaqP7Oiy9fQ4glL",
      "authorship_tag": "ABX9TyMKPT23hq3+56yW0G5j/BZf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/malijarkas/ECSE551/blob/main/Personal_Mini_Project_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Modules"
      ],
      "metadata": {
        "id": "IPcjDDU_lKQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# my imports\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "\n",
        "# henry's import\n",
        "!pip install numpy pandas matplotlib seaborn statsmodels scikit-learn autocorrect contractions tqdm spacy nltk colorama\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import statsmodels.api as sm\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from autocorrect import Speller\n",
        "import contractions\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from colorama import init, Fore, Style\n",
        "from typing import Optional\n",
        "import unicodedata\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWLUp284lOSK",
        "outputId": "21aa56e7-292d-4c64-d900-822357cab32d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (0.14.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: autocorrect in /usr/local/lib/python3.10/dist-packages (2.6.1)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.10/dist-packages (0.1.73)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (0.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.13.1)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (0.5.6)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.6->statsmodels) (1.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount drive and import data"
      ],
      "metadata": {
        "id": "rBoav8kOvPCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "train_data_path = '/content/drive/My Drive/ECSE 551 Data Set/Train.csv'\n",
        "test_data_path = '/content/drive/My Drive/ECSE 551 Data Set/Test.csv'\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "input_df = pd.read_csv(train_data_path, header=None)\n",
        "kaggle_df = pd.read_csv(test_data_path, header=None)\n",
        "\n",
        "\n",
        "# Set the column names\n",
        "input_df.columns = ['comments', 'classification']\n",
        "kaggle_df.columns = ['id', 'comments']\n",
        "\n",
        "# Get the unique classes\n",
        "class_names = input_df['classification'].unique()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sYJL0E4jM9N",
        "outputId": "4b48657d-a7ba-400b-9e29-bec2bd1cf4e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### alpha_Naive Bayes Classifier\n",
        "\n",
        "TODO:\n",
        "- fix the fixme\n",
        "- another set of eyes to make sure its correct"
      ],
      "metadata": {
        "id": "QzlY-Rt3D9BZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Naive_Bayes_Classifier:\n",
        "\n",
        "  def __init__(self, num_features, num_classes):\n",
        "    self.num_features = num_features\n",
        "    self.num_classes = num_classes\n",
        "    self.theta_k = np.zeros(num_classes)\n",
        "    self.theta_j_k = np.zeros((num_classes, num_features))\n",
        "\n",
        "  def train(self, X, Y, laplace_smoothing = True):\n",
        "    num_features = self.num_features\n",
        "    num_classes = self.num_classes\n",
        "\n",
        "    num_samples = X.shape[0]\n",
        "\n",
        "    class_frequency = np.zeros(num_classes)\n",
        "\n",
        "    # k denotes the class number\n",
        "    # j denotes the feature number\n",
        "    theta_k = np.zeros(num_classes)\n",
        "    theta_j_k = np.zeros((num_classes, num_features))\n",
        "\n",
        "    # find frequency of output class\n",
        "    for i in range (num_samples):\n",
        "       for j in range (num_classes):\n",
        "          class_index = int(Y[i])\n",
        "          class_frequency[class_index] += 1\n",
        "          break\n",
        "\n",
        "    # find P(Y = k)\n",
        "    for i in range (num_classes):\n",
        "      theta_k[i] = class_frequency[i] / num_samples\n",
        "\n",
        "    # find P(X=1|Y=k) = theta_j_k\n",
        "    # P(X=0|Y=k) = 1 - P(X=1|Y=k) = 1 - theta_j_k\n",
        "    for i in range (num_samples):\n",
        "      # FIXME: used to be X[i][0, j] until it suddenly broke? change for kfold\n",
        "      # FIX: assign to current_sample, not sure why X[i] = X[i].reshape doesnt work\n",
        "      if X[i].ndim == 1:\n",
        "        current_sample = X[i].reshape(1, -1)\n",
        "      else:\n",
        "        current_sample = X[i]\n",
        "      for j in range (num_features):\n",
        "        if (current_sample[0, j] == 1):\n",
        "          class_index = int(Y[i])\n",
        "          theta_j_k[class_index, j] += 1\n",
        "\n",
        "    if laplace_smoothing:\n",
        "      for i in range (num_classes):\n",
        "        theta_j_k[i] = (theta_j_k[i] + 1) / (class_frequency[i] + 2)\n",
        "    else:\n",
        "      for i in range (num_classes):\n",
        "        theta_j_k[i] = theta_j_k[i] / class_frequency[i]\n",
        "\n",
        "    self.theta_k = theta_k\n",
        "    self.theta_j_k = theta_j_k\n",
        "\n",
        "  def predict(self, X):\n",
        "    num_features = self.num_features\n",
        "    num_classes = self.num_classes\n",
        "    theta_k = self.theta_k\n",
        "    theta_j_k = self.theta_j_k\n",
        "\n",
        "    num_samples = X.shape[0]\n",
        "\n",
        "    delta_k = np.zeros(num_classes)\n",
        "\n",
        "    output_labels_indexed = np.zeros(num_samples, dtype=int)\n",
        "\n",
        "    for i in range (num_samples):\n",
        "\n",
        "      # calculate the delta_k for each class, iterating over the features\n",
        "      for k in range (num_classes):\n",
        "        delta_k[k] = np.log(theta_k[k])\n",
        "        for j in range (num_features):\n",
        "          if (X[i][0,j] == 1):\n",
        "            delta_k[k] += np.log(theta_j_k[k][j])\n",
        "          elif (X[i][0,j] == 0):\n",
        "            delta_k[k] += np.log(1 - theta_j_k[k][j])\n",
        "\n",
        "      #find the class with the highest delta_k\n",
        "      max_delta_k_index = np.argmax(delta_k)\n",
        "      output_labels_indexed[i] = int(max_delta_k_index)\n",
        "\n",
        "    return output_labels_indexed"
      ],
      "metadata": {
        "id": "xFUFjRoyRArG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper Functions"
      ],
      "metadata": {
        "id": "EzLd7uXk3lZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Misc functions"
      ],
      "metadata": {
        "id": "B4YbuQURcaRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# writes string array to file with delimiter '\\n'\n",
        "def _write_to_csv(output, file_name):\n",
        "  with open(file_name, 'w') as f:\n",
        "    for i in range(len(output)):\n",
        "      f.write(str(output[i]))\n",
        "      f.write('\\n')\n",
        "\n",
        "def write_to_csv_kaggle(output, file_name):\n",
        "  with open(file_name, 'w') as f:\n",
        "    f.write(\"id,subreddit\\n\")\n",
        "    for i in range(len(output)):\n",
        "      f.write(str(i))\n",
        "      f.write(',')\n",
        "      f.write(str(output[i]))\n",
        "      f.write('\\n')\n",
        "\n",
        "# Converts class names to indices, makes indexing in future operations easier\n",
        "# Takes in the Y label numpy array and outputs it indexed\n",
        "# The index values are dependant on the orders in class_names list\n",
        "def class_name_to_index(class_names, Y_label):\n",
        "    if not isinstance(Y_label, np.ndarray):\n",
        "        raise TypeError(\"Y must be a numpy array\")\n",
        "\n",
        "    Y_string = np.copy(Y_label) #to avoid modifying Y_label\n",
        "    Y_int = np.zeros(Y_string.size)\n",
        "\n",
        "    class_dict = {name: idx for idx, name in enumerate(class_names)}\n",
        "    for i in range(Y_string.size):\n",
        "        if Y_string[i] in class_dict:\n",
        "            Y_int[i] = class_dict[Y_string[i]]\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected class name '{Y_string[i]}' in Y\")\n",
        "    return np.array(Y_int)\n",
        "\n",
        "# Converts indices back to their class names\n",
        "# Takes in the same list of class names and the numpy array Y_integer\n",
        "def index_to_class_name(class_names, Y_index):\n",
        "  if not isinstance(Y_index, np.ndarray):\n",
        "    raise TypeError(\"Y_index must be a numpy array\")\n",
        "\n",
        "  Y_int = np.copy(Y_index)\n",
        "  Y_string = np.empty(Y_int.size, dtype=object)\n",
        "\n",
        "  if not (0 <= Y_int).all() and (Y_int < len(class_names)).all():\n",
        "      raise ValueError(\"Y_index parameter contains invalid class index\")\n",
        "\n",
        "  for i in range(Y_int.size):\n",
        "      Y_string[i] = class_names[int(Y_int[i])]\n",
        "  return Y_string\n",
        "\n",
        "# partitions, tokenizes, and train_test splits the data\n",
        "# returns X_train_validate, X_test, Y_train_validate, Y_test to be input into kfold\n",
        "# returns vectorizer for future tokenizing\n",
        "def preprocess_data(input_df, num_features = 3000, ngram_range = (1,1)):\n",
        "  from sklearn.naive_bayes import BernoulliNB\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "  # PLACEHOLDER begin until other preprocessing takes place\n",
        "  input_df_processed = input_df.copy()\n",
        "  # PLACEHOLDER end\n",
        "\n",
        "  input_data_x_np = np.array(input_df_processed[\"comments\"])\n",
        "  input_data_y_np = np.array(input_df_processed[\"classification\"])\n",
        "\n",
        "  # initialize vectorizer\n",
        "  vectorizer = CountVectorizer(max_features=num_features, binary=True, ngram_range = ngram_range)\n",
        "\n",
        "  # convert input_data_x to binary\n",
        "  input_data_x_bool_np = vectorizer.fit_transform(input_data_x_np).todense()\n",
        "\n",
        "  # map input data y labels to indices\n",
        "  input_data_y_indexed_np = class_name_to_index(class_names, input_data_y_np)\n",
        "\n",
        "  X_train_validate, X_test, Y_train_validate, Y_test = train_test_split(input_data_x_bool_np, input_data_y_indexed_np, test_size=0.2, random_state=42)\n",
        "\n",
        "  return X_train_validate, X_test, Y_train_validate, Y_test, vectorizer\n"
      ],
      "metadata": {
        "id": "2bOAqsel3n8t"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### K_fold related functions"
      ],
      "metadata": {
        "id": "eB6SNNeJckBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# k fold splits the data\n",
        "def _k_fold_split(input_data_x_b, input_data_y_indexed, k_folds = 10, random_state = 0):\n",
        "  # k_fold\n",
        "  kf = KFold(n_splits=k_folds, shuffle=True, random_state = random_state)\n",
        "\n",
        "  # FIXME: is this the best way to intialize them?\n",
        "  X_train = np.array([None] * k_folds)\n",
        "  X_test = np.array([None] * k_folds)\n",
        "  Y_train = np.array([None] * k_folds)\n",
        "  Y_test = np.array([None] * k_folds)\n",
        "\n",
        "  for i, (train_index, test_index) in enumerate(kf.split(input_data_x_b)):\n",
        "    X_train[i], X_test[i] = np.array(input_data_x_b[train_index]), input_data_x_b[test_index]\n",
        "    Y_train[i], Y_test[i] = input_data_y_indexed[train_index], input_data_y_indexed[test_index]\n",
        "\n",
        "  print(X_train[0].shape)\n",
        "\n",
        "  return X_train, X_test, Y_train, Y_test\n",
        "\n",
        "def k_fold_loop(X_train_validate, Y_train_validate, k = 10, compare_w_scikit = True, verbose = True):\n",
        "\n",
        "  scikit_classifiers = []\n",
        "  NB_classifiers = []\n",
        "\n",
        "  scikit_accuracies = np.zeros(k)\n",
        "  NB_accuracies = np.zeros(k)\n",
        "\n",
        "  # stores prediction labels for classifiers\n",
        "  Y_pred = np.array([None] * k)\n",
        "  if (compare_w_scikit):\n",
        "    Y_pred_scikit = np.array([None] * k)\n",
        "\n",
        "  X_train, X_validate, Y_train, Y_validate = _k_fold_split(X_train_validate, Y_train_validate, k_folds = k)\n",
        "\n",
        "  for i in range (k):\n",
        "  # initialize class, train, and predict\n",
        "    NB = Naive_Bayes_Classifier(num_features=3000, num_classes=4)\n",
        "    NB_classifiers.append(NB)\n",
        "    NB_classifiers[i].train(X_train[i], Y_train[i])\n",
        "\n",
        "    Y_pred[i] = (NB.predict(X_validate[i]))\n",
        "    NB_accuracies[i] = accuracy_score(Y_validate[i], Y_pred[i])\n",
        "\n",
        "    if (compare_w_scikit):\n",
        "      CL = BernoulliNB()\n",
        "      scikit_classifiers.append(CL)\n",
        "      scikit_classifiers[i].fit(X_train[i], Y_train[i])\n",
        "\n",
        "      Y_pred_scikit[i] = scikit_classifiers[i].predict(np.asarray(X_validate[i]))\n",
        "      scikit_accuracies[i] = accuracy_score(Y_validate[i], Y_pred_scikit[i])\n",
        "    if (verbose):\n",
        "      print(\"Finished Test: \" + str(i) + \" with accuracy: \" + str(NB_accuracies[i]))\n",
        "  if (compare_w_scikit):\n",
        "    return NB_classifiers, NB_accuracies, scikit_classifiers, scikit_accuracies\n",
        "  return NB_classifiers, NB_accuracies\n",
        "\n"
      ],
      "metadata": {
        "id": "1KrraFVuaLOJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Henry's preprocess function"
      ],
      "metadata": {
        "id": "ff4Rg051q4sh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Henry's function\n",
        "class TextPreProcessor:\n",
        "    def __init__(self, lowercase=True, remove_html=True, remove_urls=True, remove_emails=True,\n",
        "                 normalize_unicode=True, remove_punctuation=True, remove_numbers=True,\n",
        "                 remove_special_characters=True, remove_mentions_hashtags=True,\n",
        "                 correct_spelling=False, expand_contractions=True, remove_duplicates=True,\n",
        "                 remove_extra_whitespace=True, drop_missing=True, remove_short_comments=True,\n",
        "                 min_length=5):\n",
        "        self.lowercase = lowercase\n",
        "        self.remove_html = remove_html\n",
        "        self.remove_urls = remove_urls\n",
        "        self.remove_emails = remove_emails\n",
        "        self.normalize_unicode = normalize_unicode\n",
        "        self.remove_punctuation = remove_punctuation\n",
        "        self.remove_numbers = remove_numbers\n",
        "        self.remove_special_characters = remove_special_characters\n",
        "        self.remove_mentions_hashtags = remove_mentions_hashtags\n",
        "        self.correct_spelling = correct_spelling\n",
        "        self.expand_contractions = expand_contractions\n",
        "        self.remove_duplicates = remove_duplicates\n",
        "        self.remove_extra_whitespace = remove_extra_whitespace\n",
        "        self.drop_missing = drop_missing\n",
        "        self.remove_short_comments = remove_short_comments\n",
        "        self.min_length = min_length\n",
        "        self.spell = Speller()\n",
        "\n",
        "    def preprocess(self, original_df, column='comments'):\n",
        "        df = original_df.copy()\n",
        "\n",
        "        # Wrapping each step in tqdm to show progress\n",
        "        if self.lowercase:\n",
        "            tqdm.pandas(desc=\"Lowercasing\")\n",
        "            df[column] = df[column].progress_apply(lambda x: x.lower() if isinstance(x, str) else x)\n",
        "\n",
        "        if self.remove_html:\n",
        "            tqdm.pandas(desc=\"Removing HTML tags\")\n",
        "            df[column] = df[column].progress_apply(lambda x: re.sub(r'<.*?>', '', x) if isinstance(x, str) else x)\n",
        "\n",
        "        if self.remove_urls:\n",
        "            tqdm.pandas(desc=\"Removing URLs\")\n",
        "            df[column] = df[column].progress_apply(lambda x: re.sub(r'http\\S+|www\\S+|https\\S+', '', x) if isinstance(x, str) else x)\n",
        "\n",
        "        if self.remove_emails:\n",
        "            tqdm.pandas(desc=\"Removing Emails\")\n",
        "            df[column] = df[column].progress_apply(lambda x: re.sub(r'\\S+@\\S+', '', x) if isinstance(x, str) else x)\n",
        "\n",
        "        if self.normalize_unicode:\n",
        "            tqdm.pandas(desc=\"Normalizing Unicode\")\n",
        "            df[column] = df[column].progress_apply(lambda x: unicodedata.normalize('NFKD', x).encode('ascii', 'ignore').decode('utf-8') if isinstance(x, str) else x)\n",
        "\n",
        "        if self.remove_punctuation:\n",
        "            tqdm.pandas(desc=\"Removing Punctuation\")\n",
        "            df[column] = df[column].progress_apply(lambda x: re.sub(r'[^\\w\\s]', '', x) if isinstance(x, str) else x)\n",
        "\n",
        "        if self.remove_numbers:\n",
        "            tqdm.pandas(desc=\"Removing Numbers\")\n",
        "            df[column] = df[column].progress_apply(lambda x: re.sub(r'\\d+', '', x) if isinstance(x, str) else x)\n",
        "\n",
        "        if self.remove_special_characters:\n",
        "            tqdm.pandas(desc=\"Removing Special Characters\")\n",
        "            df[column] = df[column].progress_apply(lambda x: re.sub(r'[^\\x00-\\x7F]+', '', x) if isinstance(x, str) else x)\n",
        "\n",
        "        if self.remove_mentions_hashtags:\n",
        "            tqdm.pandas(desc=\"Removing Mentions and Hashtags\")\n",
        "            df[column] = df[column].progress_apply(lambda x: re.sub(r'@\\w+|#\\w+', '', x) if isinstance(x, str) else x)\n",
        "\n",
        "        if self.correct_spelling:\n",
        "            tqdm.pandas(desc=\"Correcting Spelling\")\n",
        "            df[column] = df[column].progress_apply(lambda x: self.spell(x) if isinstance(x, str) else x)\n",
        "\n",
        "        if self.expand_contractions:\n",
        "            tqdm.pandas(desc=\"Expanding Contractions\")\n",
        "            df[column] = df[column].progress_apply(lambda x: contractions.fix(x) if isinstance(x, str) else x)\n",
        "\n",
        "        if self.remove_extra_whitespace:\n",
        "            tqdm.pandas(desc=\"Removing Extra Whitespace\")\n",
        "            df[column] = df[column].progress_apply(lambda x: re.sub(r'\\s+', ' ', x) if isinstance(x, str) else x)\n",
        "\n",
        "        if self.remove_duplicates:\n",
        "            print(\"Removing Duplicates\")\n",
        "            df = df.drop_duplicates(subset=[column], keep='first')\n",
        "\n",
        "        if self.drop_missing:\n",
        "            print(\"Dropping Missing Values\")\n",
        "            df = df.dropna(subset=[column])\n",
        "\n",
        "        if self.remove_short_comments:\n",
        "            print(\"Removing Short Comments\")\n",
        "            df = df[df[column].str.len() > self.min_length]\n",
        "\n",
        "        return df"
      ],
      "metadata": {
        "id": "PejtVa-9q7rc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Henry's post process function"
      ],
      "metadata": {
        "id": "L31DcySpyqLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextPostProcessor:\n",
        "    def __init__(self, remove_stopwords=True, stem=True, lemmatize=True, pos_tag=True, ner=True, vectorize=True):\n",
        "        \"\"\"Initialize the text processor with desired processing steps.\"\"\"\n",
        "        self.remove_stopwords = remove_stopwords\n",
        "        self.stem = stem\n",
        "        self.lemmatize = lemmatize\n",
        "        self.pos_tag = pos_tag\n",
        "        self.ner = ner\n",
        "        self.vectorize = vectorize\n",
        "\n",
        "        # Initialize NLP tools\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.stemmer = PorterStemmer()\n",
        "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    def _process_text(self, tokens):\n",
        "        \"\"\"Process a single text document through the pipeline.\"\"\"\n",
        "        if not tokens:\n",
        "            return []\n",
        "\n",
        "        # Convert tokens to text for spaCy processing\n",
        "        text = ' '.join(tokens)\n",
        "        doc = self.nlp(text)\n",
        "\n",
        "        processed_tokens = tokens\n",
        "\n",
        "        # Remove stopwords first to reduce processing load\n",
        "        if self.remove_stopwords:\n",
        "            processed_tokens = [token for token in processed_tokens\n",
        "                              if token.lower() not in self.stop_words]\n",
        "\n",
        "        # Lemmatize before stemming (if both are enabled)\n",
        "        # This is more efficient as lemmatization can reduce the number of tokens\n",
        "        if self.lemmatize:\n",
        "            processed_tokens = [token.lemma_ for token in doc]\n",
        "\n",
        "        # Apply stemming after lemmatization\n",
        "        if self.stem:\n",
        "            processed_tokens = [self.stemmer.stem(token) for token in processed_tokens]\n",
        "\n",
        "        # POS tagging\n",
        "        if self.pos_tag:\n",
        "            processed_tokens = nltk.pos_tag(processed_tokens)\n",
        "            # Extract just the tokens if we need to continue processing\n",
        "            if self.ner:\n",
        "                processed_tokens = [token for token, _ in processed_tokens]\n",
        "\n",
        "        # Add NER results\n",
        "        if self.ner:\n",
        "            ner_entities = [ent.text for ent in doc.ents]\n",
        "            processed_tokens.extend(ner_entities)\n",
        "\n",
        "        return processed_tokens\n",
        "\n",
        "    def postprocess(self, df, column='comments'):\n",
        "        \"\"\"Process all texts in the dataframe.\"\"\"\n",
        "        if not isinstance(df, pd.DataFrame):\n",
        "            raise TypeError(\"Input must be a pandas DataFrame\")\n",
        "\n",
        "        if column not in df.columns:\n",
        "            raise ValueError(f\"Column '{column}' not found in DataFrame\")\n",
        "\n",
        "        # Create a copy to avoid modifying the original\n",
        "        processed_df = df.copy()\n",
        "\n",
        "        # Process all comments\n",
        "        tqdm.pandas(desc=\"Processing texts\")\n",
        "        processed_df[column] = processed_df[column].progress_apply(self._process_text)\n",
        "\n",
        "        # Vectorize the processed comments\n",
        "        if self.vectorize:\n",
        "            tqdm.pandas(desc=\"Vectorizing\")\n",
        "            # Join tokens back into strings for vectorization\n",
        "            text_for_vectorization = processed_df[column].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
        "\n",
        "            vectorizer = TfidfVectorizer()\n",
        "            vectorized_comments = vectorizer.fit_transform(text_for_vectorization)\n",
        "\n",
        "            # Store the vocabulary and feature names\n",
        "            self.vectorizer_vocabulary_ = vectorizer.vocabulary_\n",
        "            self.feature_names_ = vectorizer.get_feature_names_out()\n",
        "\n",
        "            # Add vectorized results as a new column\n",
        "            processed_df['vectorized_comments'] = list(vectorized_comments.toarray())\n",
        "\n",
        "        return processed_df"
      ],
      "metadata": {
        "id": "hiRwOxB-ys4A"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### main"
      ],
      "metadata": {
        "id": "QJuB1IHYsa0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_validate, X_test, Y_train_validate, Y_test, vectorizer = preprocess_data(input_df, 3000, ngram_range = (1,1))\n",
        "\n",
        "NB_classifiers, NB_accuracies, scikit_classifiers, scikit_accuracies = k_fold_loop(X_train_validate, Y_train_validate)"
      ],
      "metadata": {
        "id": "VFWAxHa93b8B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1dc7c24-ffdd-4825-a141-6e655c8b0ec8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1008, 3000)\n",
            "Finished Test: 0 with accuracy: 0.5625\n",
            "Finished Test: 1 with accuracy: 0.4732142857142857\n",
            "Finished Test: 2 with accuracy: 0.5089285714285714\n",
            "Finished Test: 3 with accuracy: 0.48214285714285715\n",
            "Finished Test: 4 with accuracy: 0.49107142857142855\n",
            "Finished Test: 5 with accuracy: 0.49107142857142855\n",
            "Finished Test: 6 with accuracy: 0.48214285714285715\n",
            "Finished Test: 7 with accuracy: 0.42857142857142855\n",
            "Finished Test: 8 with accuracy: 0.4375\n",
            "Finished Test: 9 with accuracy: 0.5535714285714286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracies \")\n",
        "\n",
        "for i in range (len(NB_accuracies)):\n",
        "  print(\"--------------------------------\")\n",
        "  print(\"NB accuracy for test \" + str(i) + \": \" + str(NB_accuracies[i]))\n",
        "  print(\"Scikit accuracy for test \" + str(i) + \": \" + str(scikit_accuracies[i]))\n",
        "\n",
        "print(\"**********************************\")\n",
        "print(\"Average NB accuracy\", np.mean(NB_accuracies))\n",
        "print(\"Average Scikit accuracy\", np.mean(scikit_accuracies))\n",
        "\n",
        "### ok, now retrain over test+validate data and predict over my test data and see if the accuracies match\n",
        "Nb = Naive_Bayes_Classifier(num_features=3000, num_classes=4)\n",
        "\n",
        "Nb.train(X_train_validate, Y_train_validate)\n",
        "Y_pred_final = Nb.predict(X_test)\n",
        "print(\"Final accuracy: \" + str(accuracy_score(Y_test, Y_pred_final)))\n",
        "\n",
        "Nb_sci = BernoulliNB()\n",
        "Nb_sci.fit(np.asarray(X_train_validate), np.asarray(Y_train_validate))\n",
        "Y_pred_final_sci = Nb_sci.predict(np.asarray(X_test))\n",
        "\n",
        "print(\"Final scikit accuracy: \" + str(accuracy_score(Y_test, Y_pred_final_sci)))\n",
        "\n",
        "\n",
        "# kaggle_label_indexed = NB_classifiers[highest_accuracy_index].predict(test_data_kaggle_b)\n",
        "# # kaggle_scikit_label_indexed = scikit_classifiers[highest_scikit_accuracy_index].predict(np.asarray(test_data_kaggle_b))\n",
        "\n",
        "# kaggle_label = index_to_class_name(class_names, kaggle_label_indexed)\n",
        "# # kaggle_scikit_label = index_to_class_name(class_names, kaggle_scikit_label_indexed)\n",
        "\n",
        "# write_to_csv_kaggle(kaggle_label, \"kaggle_predictions.csv\")\n",
        "\n",
        "#\n",
        "\n",
        "# # ### these need to be reshuffled... thats why final predictions are low\n",
        "# # ### ^^^ im not sure of the truth of this\n",
        "# final_label = NB_classifiers[highest_accuracy_index].predict(input_data_x_b)\n",
        "# final_scikit_label = scikit_classifiers[highest_scikit_accuracy_index].predict(np.asarray(input_data_x_b))\n",
        "# print(\"final prediction using highest accuracy model\")\n",
        "# print(\"for my model:\")\n",
        "# print(calculate_accuracy(final_label, input_data_y_indexed))\n",
        "# print(\"for scikit model:\")\n",
        "# print(calculate_accuracy(final_scikit_label, input_data_y_indexed))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEAFBXa3ZcN6",
        "outputId": "caa1965f-143c-4512-9078-5dbf96f501ec"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracies \n",
            "--------------------------------\n",
            "NB accuracy for test 0: 0.5625\n",
            "Scikit accuracy for test 0: 0.5625\n",
            "--------------------------------\n",
            "NB accuracy for test 1: 0.4732142857142857\n",
            "Scikit accuracy for test 1: 0.4732142857142857\n",
            "--------------------------------\n",
            "NB accuracy for test 2: 0.5089285714285714\n",
            "Scikit accuracy for test 2: 0.5089285714285714\n",
            "--------------------------------\n",
            "NB accuracy for test 3: 0.48214285714285715\n",
            "Scikit accuracy for test 3: 0.48214285714285715\n",
            "--------------------------------\n",
            "NB accuracy for test 4: 0.49107142857142855\n",
            "Scikit accuracy for test 4: 0.49107142857142855\n",
            "--------------------------------\n",
            "NB accuracy for test 5: 0.49107142857142855\n",
            "Scikit accuracy for test 5: 0.49107142857142855\n",
            "--------------------------------\n",
            "NB accuracy for test 6: 0.48214285714285715\n",
            "Scikit accuracy for test 6: 0.48214285714285715\n",
            "--------------------------------\n",
            "NB accuracy for test 7: 0.42857142857142855\n",
            "Scikit accuracy for test 7: 0.42857142857142855\n",
            "--------------------------------\n",
            "NB accuracy for test 8: 0.4375\n",
            "Scikit accuracy for test 8: 0.4375\n",
            "--------------------------------\n",
            "NB accuracy for test 9: 0.5535714285714286\n",
            "Scikit accuracy for test 9: 0.5535714285714286\n",
            "**********************************\n",
            "Average NB accuracy 0.49107142857142866\n",
            "Average Scikit accuracy 0.49107142857142866\n",
            "Final accuracy: 0.475\n",
            "Final scikit accuracy: 0.475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### old main"
      ],
      "metadata": {
        "id": "HQ6oxKyjc2S6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "# preprocessor = TextPreProcessor(lowercase=True,\n",
        "#                                 remove_html=1,\n",
        "#                                 remove_urls=1,\n",
        "#                                 remove_emails=1,\n",
        "#                                 normalize_unicode=1,\n",
        "#                                 remove_punctuation=1,\n",
        "#                                 remove_numbers=1,\n",
        "#                                 remove_special_characters=1,\n",
        "#                                 remove_mentions_hashtags=1,\n",
        "#                                 correct_spelling=0,\n",
        "#                                 expand_contractions=1,\n",
        "#                                 remove_duplicates=0,\n",
        "#                                 remove_extra_whitespace=1,\n",
        "#                                 drop_missing=0,\n",
        "#                                 remove_short_comments=0, min_length=5,)\n",
        "\n",
        "# postprocessor = TextPostProcessor(remove_stopwords=True, stem=True, lemmatize=False, pos_tag=False, ner=True, vectorize=True)\n",
        "\n",
        "\n",
        "# processed_input_df = preprocessor.preprocess(train_df, column='comments')\n",
        "# processed_test_df = preprocessor.preprocess(kaggle_test_df, column='comments')\n",
        "# processed_input_df = postprocessor.postprocess(processed_input_df, column='comments')\n",
        "# processed_input_df = train_df.copy()\n",
        "# print(processed_input_df)\n",
        "\n",
        "\n",
        "kaggle_df_processed = kaggle_df.copy()\n",
        "X_kaggle_np= np.array(kaggle_df_processed[\"comments\"][1:])\n",
        "\n",
        "\n",
        "# convert input_data_x to binary\n",
        "\n",
        "X_kaggle_bool_np = vectorizer.transform(X_kaggle_np).todense()\n",
        "\n",
        "# run the main k fold train/test loop, compare with scikit naive bayes\n",
        "\n",
        "\n",
        "# run the main k fold train/test loop, compare with scikit naive bayes\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oEhw6dADc1gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### test_train_split main"
      ],
      "metadata": {
        "id": "J43GCl1FmIMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "write_to_csv_kaggle(kaggle_label, \"kaggle_predictions.csv\")\n",
        "print(kaggle_label)\n"
      ],
      "metadata": {
        "id": "HW0gFuoD7tfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  NB = Naive_Bayes_Classifier(num_features=3000, num_classes=4)\n",
        "  NB.train(X_train, Y_train)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZbtXyh_NmLQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  NB.latest_output_indexed = NB.predict(X_test, Y_test)\n"
      ],
      "metadata": {
        "id": "EeZbX-u0qpsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  NB.accuracy = calculate_accuracy(NB.latest_output_indexed, Y_test)\n",
        "  print(NB.accuracy)"
      ],
      "metadata": {
        "id": "p-lmEIx_ngDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Outdated"
      ],
      "metadata": {
        "id": "ovQt8kl08ton"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # converts class names to integers to be used as indices\n",
        "# # takes in a list of class names and the numpy array Y\n",
        "# def class_name_to_index(class_names, Y):\n",
        "#     for i in range (Y.shape[0]):\n",
        "#         for j in range (class_names.shape[0]):\n",
        "#           if Y[i] == class_names[j]:\n",
        "#             Y[i] = j\n",
        "#             break\n",
        "#           # elif j == class_names.shape[0] - 1:\n",
        "#           #   print(Y[i])\n",
        "#           #   print(j)\n",
        "#           #   raise ValueError('unexpected class name in Y')\n",
        "#     return Y\n",
        "\n",
        "# # converts class index (i.e. integers) back to their class names\n",
        "# # takes in the same list of class names and the numpy array Y_integer\n",
        "# def index_to_class_name(class_names, Y_integer):\n",
        "#     for i in range (Y_integer.shape[0]):\n",
        "#         for j in range (class_names.shape[0]):\n",
        "#           if Y_integer[i] == j:\n",
        "#             Y_integer[i] = class_names[j]\n",
        "#             break\n",
        "#           elif j == class_names.shape[0] - 1:\n",
        "#             raise ValueError('unexpected class index in Y_integer')\n",
        "#     return Y_integer\n",
        "\n",
        "# for i in range (test_data_y.shape[0]):\n",
        "#   print(test_data_y[i])\n",
        "#   if test_data_y[i] == 'Toronto':\n",
        "#     Y_test[i] = 0\n",
        "#   elif test_data_y[i] == 'Montreal':\n",
        "#     Y_test[i] = 1\n",
        "#   elif test_data_y[i] == 'London':\n",
        "#     Y_test[i] = 2\n",
        "#   elif test_data_y[i] == 'Brussels':\n",
        "#     Y_test[i] = 3\n",
        "\n"
      ],
      "metadata": {
        "id": "JVr3Y5xO8vLV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}